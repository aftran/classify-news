#!/usr/bin/env python
"""
Blank out duplicate lines within a newsgroup given an index of duplicate text.

Usage: deduplicate_20news.py duplicates_index corpus_dir

Rather than running this file directly, it might be easier to run
deduplicate_20news.sh, which generates an appropriate duplicates_index for the
corpus.  Otherwise, the duplicates_index should be generated by changing to
corpus_dir and running

  sim_text -Tn $newsgroup/* >> duplicates_index

for each newsgroup (=subdirectory) in the corpus.

output_dir must exist.

This only approximates duplicate text deletion, because if a piece of text
appears more than twice, it might be deleted altogether.
"""
import sys, re, email
import os
from os import path
from subprocess import call

"""Regex for identifying the last line of a header block."""
# LAST_HEADER_REGEX = r'^Lines: \d+$' # misses 15 files
LAST_HEADER_REGEX = r'^$' # probably works perfectly

def last_header(filepath):
  """
  Return the line the headers probably end at in a 20_newsgroups file.
  
  Assumes each header block ends with a line that matches LAST_HEADER_REGEX.
  """
  with open(filepath) as fi:
    for num, line in enumerate(fi, 1):
      if re.match(LAST_HEADER_REGEX, line):
        return num
  print >> sys.stderr, 'Warning: no "%s" line found in %s' % (LAST_HEADER_REGEX,
                                                              filepath)
  return 0



def empty_payload(msg_path):
  """Return whether the email at msg_path has a whitespace-only payload."""
  with open(msg_path) as f:
    msg = email.message_from_file(f)
  if msg.get_payload().strip() == '':
    return True
  else:
    return False



def main():
  dupsfile, outdir = sys.argv[1:]
  with open(dupsfile) as f:
    for line in f:
      match = re.match('([^/]+/\d+): line (\d+)-(\d+)\s*\|.*$', line)
      if match:
        doc        = match.group(1)
        start_line = int(match.group(2))
        end_line   = int(match.group(3))
        doc_path   = path.join(outdir, doc)

        # Don't blank out headers, even if they're duplicates.  But DO blank out
        # any non-header text that's part of this range.
        last_header_line = last_header(doc_path)
        if end_line > last_header_line:
          if start_line < last_header_line:
            start_line = last_header_line + 1

          result = call(['sed',
                         '-i',
                         '%s,%ss/.*//' % (start_line, end_line),
                         doc_path])
          if result > 0:
            print >> sys.stderr, 'Trouble with running sed on %s' % doc_path

  # Delete files that are nothing but line breaks.  We don't currently delete
  # files that contain only headers.
  for folder, _, filenames in os.walk(outdir):
    for filename in filenames:
      filepath = path.join(folder, filename)
      if empty_payload(filepath):
        os.remove(filepath)
        print 'Deleted empty email ' + filepath



if __name__ == '__main__': main()
