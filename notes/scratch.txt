Scratch pad.  See README for a cleaner version of many of these thoughts.


Examine data set
================
Get counts for classes
- 1000 per class.
- That means there's no difference between micro- and macro-averaging during evaluation.
- And we'll have a uniform class prior, unless we think of some clever prior instead.

Split out a test set at random and don't look at it, if only for being able to believably say that our performance should generalize beyond the training set.

Look at data and see if any obvious features leap out for initial attempt
- Maybe replacing names with NAME.

There ARE attachments we'll want to remove.  Though maybe at worst they'll just result in a bunch of one-hot features that bloat the model.



Training/feature engineering
============================
ML pipeline, parametrized by feature template set and, if feasible, ML algorithm.

Get article text without headers, since that won't help us with the news site, and also strip line-initial runs of ">" and ":".

In fact, quoted text ('>', ':') seems to be often (but not always) multi-represented in the corpus.
- Sloppy response: ignore those lines.  Nicer response: detect duplicates.  Let's go with the former for now, because I suspect we'd only miss a small part of the corpus.
- Complication: we probably DON'T want to ignore quoted text when classifying.  Just when training.  But see whether the extra complication would actually help.

Random idea: n-grams of POS tags.

And although this won't help for the news website, consider features like "has quoted lines", since some newsgroups might be more response-oriented versus broadcast-oriented.

Might as well start with naive Bayes, SVM and maxent.

For SVM and maxent, need to review multiclass for SVMs: http://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html
- OneVsRestClassifier

Linear SVM is really slow, even with just 100 features.  Training is maybe quadratic in number of documents.  So I'll only return to SVM if I can't get other methods to perform well.  It might be worth looking for the SVMs intended for sparse data sets; I think they exist.

DONE Figure out what to do about the fact that if we have bag-of-words features, we'll have massive vectors.  Oh!  See 6.2.3. Text feature extraction, which might require a later version of sklearn.

Expect to want different feature sets with each

Starting with a small set of features then do error analysis to find more, ideally look for cases where system is blind to something relevant that we can notice and encode as a feature

See whether the scaling, which we do in case the estimator is SVM, hurts non-SVM estimators.



Testing
=======
<s>Probably want to macro-average, not micro-average.</s> (True but doesn't matter in this test set.)
Consider looking at feature weights and removing ones with low ones.
I notice there's sklearn.svm.libsvm.cross_validation.
- And 8.25.1. sklearn.pipeline.PipelineÂ¶.

Warning!  We'll need to make sure our testing isn't messed up by the fact that messages quote each other.  Even more reason to just ignore quotes?  I think there was a paper about this mentioned in IR class.  

Warning (more important)!  Chris's book says of this corpus, "After the removal of duplicate articles, as it is usually used, it contains 18941 articles."  We absolutely must remove duplicate articles.  Asking Gummi if I can use a published deduplicated version of the corpus.
- Decided to deal with it myself based on his response.

Proposed policies for all duplicated text (might only bother to deal with it on the file level for now):
- If both duplicates are in the same class, either train on both or test on both.
- If the duplicates are between classes, I don't think we need to do anything special.  This will decrease our evaluation metrics, but that might give us some healthy encouragement to avoid overfitting.

Proposed way to approximate these policies:
- For pair of duplicated line ranges, blank out or ignore one of them.  But only do this when they're within the same newsgroup; it's okay to have cross-newsgroup duplication.
- This could theoretically cause us to completely eradicate a bit of text from the corpus, but I'm guessing that's rare, so I'm going to not care about that right now.
- The easiest way to implement this is probably to take the first column of sim_text's output and blank the line ranges mentioned there.  So we'd be generating our own copy of the corpus.
- This will probably delete various headers.  That's fine unless I'm using headers for features.  We can do a recursive diff to see what the damage is, anyway.
- Then delete the files that have become completely blank, just so they don't mess up our train/test ratios.
- I'm currently doing all this by using Python to parse the output of sim_text and generate a bunch of sed commands that collectively generate a new corpus.  Very messy.

To avoid deduplicating headers, we assume the last header line is '^Lines: [0-9]+$'.
- Only fifteen documents are missing that line.  I don't really care about such a small number of potential (not guaranteed) errors.
- Of the documents that have that line, it's always early (<18), which makes me rather confident that that is in fact where the headers end.
- Oh, oops, it seems "the first blank line is actually better, based on glancing at the distribution of where the first blank line is.

With bag-of-words and bigram features, accuracy went from 90s to 70s when I got rid of headers.
- This isn't too surprising, because there are probably a lot of shared subject lines and email addresses within a newsgroup, so we were probably overfitting when we had access to the headers.

Porter stemming is a little bit slow in the vectorization phase.  But it still finishes after a few minutes.

I'd like to use Porter2 (Snowball), but my version of NLTK lacks ...snowball.EnglishStemmer?!

Okay, I realize my classifiers should be subclasses, not modules, so that the user can rely on the fields.



Observations about results
==========================
When I fixed the bug that stopped me from properly deleting empty emails in the deduplicated corpus, NB went up slightly to 80% and linSVC went down from 60 to 58.
- I'm surprised in the linSVC getting worse, actually, because you'd think it would be improved without all those zero vectors.  Which makes me think more regularization might be a good idea.

Maxent with stemmed unigrams was only 0.749577940349.
With stemmed unigrams AND bigrams: 0.7614

NB with POS 1,2-grams: 0.79628587507
Alpha up to 10, POS trigrams et stem uni,bi: 0.821046707935
And get rid of POS tags, leaving JUST uni,bi stems, and we get 0.832301631964.

LinearSVC 0.754642656162 now that we no longer do the wrong standardization.  Ideally we should do the correct standardization, though.

Correcting the standardization mistake actually seriously hurts naive Bayes.  This means naive Bayes works better when the components have had their variances divided out...   but NOT when the test docs have the same correction applied?
- So let's try IDF weighting.  Well, that didn't HURT, until we introduced POS trigrams...

But TF-IDF causes LinearSVC to now perform nicely, at 0.835115362971!
Sticking with 0.83455261677 for using stems

Well, with TF-IDF and just bag-of-stems, KNN gets 0.707934721441.  Adding more features seems to make it perform horribly.




Part 2: classifying document at URL
===================================
Using requests library, but Ubuntu's is horribly out of date, so installing with pip.
Can use joblib to serialize model and cloud (on pip) to serialize vectorizer.

Consider asking Gummi if we should try to detect when an article fits none of those classes.  (Not entirely sure yet how we'd do that yet.)

Strip HTML and consider paying attention to document fields, like considering the title to have appeared twice
Manually optimize for good field weights (if time is running out, ask Gummi whether he'd like to see me do this, or it's enough that I mentioned it's possible)
Hm, since this would require constructing our own test set, maybe they don't really need to see us optimize this, and it's more about being able to download and clean up HTTP data.


Handoff
=======
Pickle the models
Write usage instruction
Could make doc strings more conformant if that's a good use of time.
Okay, just hours from the end of my work, I did glance at http://www-nlp.stanford.edu/wiki/Software/Classifier/20_Newsgroups to see how my performance compared to others (basically, we're almost 10 percentage points below the best models mentioned on that page).
