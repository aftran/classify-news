Examine data set
================
Get counts for classes
- 1000 per class.
- That means there's no difference between micro- and macro-averaging during evaluation.
- And we'll have a uniform class prior, unless we think of some clever prior instead.

Split out a test set at random and don't look at it, if only for being able to believably say that our performance should generalize beyond the training set.

Look at data and see if any obvious features leap out for initial attempt
- Maybe replacing names with NAME.



Training/feature engineering
============================
ML pipeline, parametrized by feature template set and, if feasible, ML algorithm.

Get article text without headers, since that won't help us with the news site, and also strip line-initial runs of ">" and ":".

In fact, quoted text ('>', ':') seems to be often (but not always) multi-represented in the corpus.
- Sloppy response: ignore those lines.  Nicer response: detect duplicates.  Let's go with the former for now, because I suspect we'd only miss a small part of the corpus.
- Complication: we probably DON'T want to ignore quoted text when classifying.  Just when training.  But see whether the extra complication would actually help.

Random idea: n-grams of POS tags.

And although this won't help for the news website, consider features like "has quoted lines", since some newsgroups might be more response-oriented versus broadcast-oriented.

Might as well start with naive Bayes, SVM and maxent.

For SVM and maxent, need to review multiclass for SVMs: http://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html
- OneVsRestClassifier

Linear SVM is really slow, even with just 100 features.  Training is maybe quadratic in number of documents.  So I'll only return to SVM if I can't get other methods to perform well.  It might be worth looking for the SVMs intended for sparse data sets; I think they exist.

DONE Figure out what to do about the fact that if we have bag-of-words features, we'll have massive vectors.  Oh!  See 6.2.3. Text feature extraction, which might require a later version of sklearn.

Expect to want different feature sets with each

Starting with a small set of features then do error analysis to find more, ideally look for cases where system is blind to something relevant that we can notice and encode as a feature

See whether the scaling, which we do in case the estimator is SVM, hurts non-SVM estimators.



Testing
=======
<s>Probably want to macro-average, not micro-average.</s> (True but doesn't matter in this test set.)
Consider looking at feature weights and removing ones with low ones.
I notice there's sklearn.svm.libsvm.cross_validation.
- And 8.25.1. sklearn.pipeline.PipelineÂ¶.

Warning!  We'll need to make sure our testing isn't messed up by the fact that messages quote each other.  Even more reason to just ignore quotes?  I think there was a paper about this mentioned in IR class.  

Warning (more important)!  Chris's book says of this corpus, "After the removal of duplicate articles, as it is usually used, it contains 18941 articles."  We absolutely must remove duplicate articles.  Asking Gummi if I can use a published deduplicated version of the corpus.
- Decided to deal with it myself based on his response.

Proposed policies for all duplicated text (might only bother to deal with it on the file level for now):
- If both duplicates are in the same class, either train on both or test on both.
- If the duplicates are between classes, I don't think we need to do anything special.  This will decrease our evaluation metrics, but that might give us some healthy encouragement to avoid overfitting.

Proposed way to approximate these policies:
- For pair of duplicated line ranges, blank out or ignore one of them.  But only do this when they're within the same newsgroup; it's okay to have cross-newsgroup duplication.
- This could theoretically cause us to completely eradicate a bit of text from the corpus, but I'm guessing that's rare, so I'm going to not care about that right now.
- The easiest way to implement this is probably to take the first column of sim_text's output and blank the line ranges mentioned there.  So we'd be generating our own copy of the corpus.
- This will probably delete various headers.  That's fine unless I'm using headers for features.  We can do a recursive diff to see what the damage is, anyway.
- Then delete the files that have become completely blank, just so they don't mess up our train/test ratios.
- I'm currently doing all this by using Python to parse the output of sim_text and generate a bunch of sed commands that collectively generate a new corpus.  Very messy.

To avoid deduplicating headers, we assume the last header line is '^Lines: [0-9]+$'.
- Only fifteen documents are missing that line.  I don't really care about such a small number of potential (not guaranteed) errors.
- Of the documents that have that line, it's always early (<18), which makes me rather confident that that is in fact where the headers end.
- Oh, oops, it seems "the first blank line is actually better, based on glancing at the distribution of where the first blank line is.





Part 2: classifying document at URL
===================================
Ask Gummi if we should try to detect when an article fits none of those classes.  (Not entirely sure yet how to do that yet.)
Strip HTML and consider paying attention to document fields, like considering the title to have appeared twice
Manually optimize for good field weights (if time is running out, ask Gummi whether he'd like to see me do this, or it's enough that I mentioned it's possible)
Hm, since this would require constructing our own test set, maybe they don't really need to see us optimize this, and it's more about being able to download and clean up HTTP data.


Handoff
=======
Pickle the models
Write usage instruction
Could make doc strings more conformant if that's a good use of time.
